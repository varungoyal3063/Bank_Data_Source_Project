{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dbaa94-5fdd-4bcc-b8d3-3ae2d33c7ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in c:\\users\\adashrat\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f893e8-e4ed-4a67-9801-a900346c47b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.235.30.200:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ebf5740b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98590728-861e-426f-811a-32e186a80bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import SparkContext\n",
    "from pyspark import SparkContext\n",
    "# Step 2: Initialize SparkContext (use getOrCreate to avoid multiple context errors)\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f25291-3d97-4354-abdd-905b09752051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42080d54-9de9-4453-86d1-b465e5a0860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "from pyspark.sql.types import ArrayType,DoubleType,BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbef88b-d764-45ed-87e3-feecec9a1de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|COMPANY_LOCATION|EMPLOYEE_COUNT|\n",
      "+----------------+--------------+\n",
      "|        Hyderbad|           840|\n",
      "|       Bangalore|           278|\n",
      "|           Delhi|           272|\n",
      "|     Navi Mumbai|           190|\n",
      "|          Mumbai|           188|\n",
      "|                |            99|\n",
      "|         Chennai|            83|\n",
      "|            Pune|            69|\n",
      "|           Noida|            34|\n",
      "|          Jaipur|            25|\n",
      "|    Gandhi Nagar|            21|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Region-wise Employee Count with Partitioning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load the CSV file as RDD\n",
    "rdd = sc.textFile(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Extract header and filter it out\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Repartition the RDD for parallel processing\n",
    "partitioned_rdd = data_rdd.repartition(4)\n",
    "\n",
    "# Process: Extract COMPANY LOCATION (assumed at index 5), count occurrences\n",
    "location_counts = partitioned_rdd \\\n",
    "    .map(lambda row: row.split(\",\", -1)) \\\n",
    "    .filter(lambda fields: len(fields) > 5) \\\n",
    "    .map(lambda fields: (fields[5].strip(), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "# Convert to DataFrame\n",
    "row_rdd = location_counts.map(lambda pair: Row(pair[0], str(pair[1])))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"COMPANY_LOCATION\", StringType(), True),\n",
    "    StructField(\"EMPLOYEE_COUNT\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(row_rdd, schema)\n",
    "\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14ae59f-3160-4b6f-a4e8-e36f444d24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+------------------+\n",
      "|       EMP_NAME|   INCOME|       TAX_SAVINGS|\n",
      "+---------------+---------+------------------+\n",
      "|        Hussain|  21430.0|               0.0|\n",
      "|         Nagesh|   2900.0|               0.0|\n",
      "|       Preetham|   2222.0|               0.0|\n",
      "| Rajashekarappa| 150681.0|               0.0|\n",
      "|        Monappa|1878787.0|          167878.7|\n",
      "|          Uthra| 231886.0|3188.6000000000004|\n",
      "|Narasimhamurthy| 447787.0|           24778.7|\n",
      "|         Thorat| 220202.0|            2020.2|\n",
      "|        Puvvadi|  12180.0|               0.0|\n",
      "|         Gawand|  59300.0|               0.0|\n",
      "|      Pulathota|  27000.0|               0.0|\n",
      "|      Manjunath|  39052.0|               0.0|\n",
      "|        Kandpal|  60000.0|               0.0|\n",
      "|     Pazhanivel|   5000.0|               0.0|\n",
      "|       Chappidi|   1620.0|               0.0|\n",
      "|           Jose|  22900.0|               0.0|\n",
      "|         Dsouza|   1300.0|               0.0|\n",
      "|      Gangasani|   5002.0|               0.0|\n",
      "|   Ghattamaneni|      0.0|               0.0|\n",
      "|         Ghadge|  40000.0|               0.0|\n",
      "+---------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Income Tax Savings Calculation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Define tax rate\n",
    "tax_rate = 0.10\n",
    "\n",
    "# Step 1: Remove rows with null or empty EMP_NAME\n",
    "cleaned_df = df.filter(\n",
    "    col(\"EMP_NAME\").isNotNull() & (trim(col(\"EMP_NAME\")) != \"\")\n",
    ")\n",
    "\n",
    "# Step 2: Convert BALANCE to INCOME and calculate TAX_SAVINGS\n",
    "result_df = cleaned_df \\\n",
    "    .withColumn(\"INCOME\", col(\"BALANCE\").cast(DoubleType())) \\\n",
    "    .withColumn(\"TAX_SAVINGS\",\n",
    "        when(col(\"INCOME\") > 200000, (col(\"INCOME\") - 200000) * tax_rate)\n",
    "        .otherwise(0.0)\n",
    "    ) \\\n",
    "    .select(\"EMP_NAME\", \"INCOME\", \"TAX_SAVINGS\") \\\n",
    "    .filter(col(\"INCOME\").isNotNull())\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cab593-0721-46c9-94f6-a431b2704f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-------------------+-------------------------------------------+\n",
      "|EMP_NAME       |EXPERIENCE|Experience_Category|Offer                                      |\n",
      "+---------------+----------+-------------------+-------------------------------------------+\n",
      "|Hussain        |2.0       |0-2 years          |Welcome Kit + 5% Cashback on first purchase|\n",
      "|Nagesh         |3.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Preetham       |4.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Rajashekarappa |6.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Monappa        |7.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Uthra          |8.0       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Narasimhamurthy|4.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Thorat         |3.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Puvvadi        |3.0       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Gawand         |2.0       |0-2 years          |Welcome Kit + 5% Cashback on first purchase|\n",
      "|Pulathota      |6.0       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Manjunath      |5.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Kandpal        |8.0       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Pazhanivel     |4.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Chappidi       |3.5       |2-5 years          |Silver Membership + 10% Cashback           |\n",
      "|Jose           |6.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Dsouza         |7.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Gangasani      |7.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Ghattamaneni   |6.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "|Ghadge         |5.5       |5-10 years         |Gold Membership + 15% Cashback             |\n",
      "+---------------+----------+-------------------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Experience Based Offers\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Ensure EXPERIENCE is numeric\n",
    "df = df.withColumn(\"EXPERIENCE\", col(\"EXPERIENCE\").cast(\"double\"))\n",
    "\n",
    "# Categorize experience and assign offers\n",
    "result_df = df.withColumn(\"Experience_Category\",\n",
    "    when(col(\"EXPERIENCE\") <= 2, \"0-2 years\")\n",
    "    .when(col(\"EXPERIENCE\") <= 5, \"2-5 years\")\n",
    "    .when(col(\"EXPERIENCE\") <= 10, \"5-10 years\")\n",
    "    .otherwise(\"10+ years\")\n",
    ").withColumn(\"Offer\",\n",
    "    when(col(\"Experience_Category\") == \"0-2 years\", \"Welcome Kit + 5% Cashback on first purchase\")\n",
    "    .when(col(\"Experience_Category\") == \"2-5 years\", \"Silver Membership + 10% Cashback\")\n",
    "    .when(col(\"Experience_Category\") == \"5-10 years\", \"Gold Membership + 15% Cashback\")\n",
    "    .otherwise(\"Platinum Membership + 20% Cashback + VIP Support\")\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "final_df = result_df.select(\"EMP_NAME\", \"EXPERIENCE\", \"Experience_Category\", \"Offer\")\n",
    "\n",
    "# Show the result\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18a1ac6f-ba6b-4350-be70-ab51e9184ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------+------+-------+----------------+-----------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+-------------+-------------+--------------+---------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "|       EMP_NAME|EMP_AGE|MARITAL STATUS|EMP ID|COMPANY|COMPANY LOCATION|   SPECIALIZATION|DESIGNATION_ID|  PHONE NO|BANK BRANCH|EXPERIENCE|EDUCATION|  VOTER_ID|  PAN CARD|HOUSING|Loan|LOAN TYPE_ID|REWARD POINTS |SAVINGS ACCOUNT NO.|    CD ACC NO|    FD ACC NO|    OD ACC NO|CURRENT ACC NO|  BALANCE|     CREDIT CARD NO|      DEBIT CARD NO|INTERNET BANKING|AVG BALANCE YEAR1|AVG BALANCE YEAR 2|AVG BALANCE YEAR 3|ADDRESS OF EMPLOYEES|ACCOUNT OPENING DATE|_c32|_c33|\n",
      "+---------------+-------+--------------+------+-------+----------------+-----------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+-------------+-------------+--------------+---------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "|    Chakraborty|     36|       married| 10078|    IBM|    Gandhi Nagar|              V&V|          1008|9563201478|  Bangalore|       8.0| BE/BTech|PLI9632587|GHAND5006N|    yes|  no|         208|          3625|     10025000000048|         NULL|1114020000172|         NULL|          NULL|5464527.0|4423 2385 1258 1223|1281 9438 8752 1005|              no|            64207|            358179|            151899|H.No:10/S,Worlika...|          25-06-2007|NULL|NULL|\n",
      "|      JJ Martin|     26|       married| 23013|  Wipro|        Hyderbad|Real Time Systems|          1003|9123458837|   Hyderbad|       4.0| BE/BTech|IJP8252378|HYBRD9684T|    yes|  no|         208|           125|     10025000000806|         NULL|         NULL|         NULL|          NULL|5464527.0|5030 2385 1258 1223|2039 9438 8752 1005|             yes|           128862|            320316|            427183|H.No:4/J,derashar...|          25-07-2012|NULL|NULL|\n",
      "|       WV Raman|     29|       married| 22402|    TCS|        Hyderbad|          Red Hat|          1010|9123458665|   Hyderbad|       7.0| BE/BTech| EFU589640|HYBRD9641W|    yes|  no|         208|            35|     10025000000763|1112510000723|1114020000824|1111250000292|          NULL|1878787.0|4995 2385 1258 1223|1996 9438 8752 1005|             yes|           133071|            224638|            595816|H.No:5/D,savla st...|          26-05-2009|NULL|NULL|\n",
      "|        Monappa|     30|        single| 10035|    IBM|       Bangalore|           DotNet|          1008|9845635674|  Bangalore|       7.5| BE/BTech| ZAG614233|BJNPA6875G|     no|  no|         208|          5214|     10025000000005|1112510000252|         NULL|         NULL|          NULL|1878787.0|4389 2385 1258 1223|1238 9438 8752 1005|             yes|           109832|            141143|             18386|H.No:2/T,Chandvar...|          01-03-2009|NULL|NULL|\n",
      "|       Phadtare|     33|       married| 10024|    IBM|            Pune|             J2EE|          1008|9861131266|  Bangalore|       5.5| BE/BTech| APL784516|PUENP7011Y|    yes|  no|         208|          2563|     10025000000084|1112510000287|         NULL|         NULL| 1111010000758| 821163.0|4452 2385 1258 1223|1317 9438 8752 1005|             yes|             4807|            305204|            340341|H.No:7/S,shivari ...|          25-08-2008|NULL|NULL|\n",
      "|Y Venugopal Rao|     30|        single|232311|  Wipro|        Hyderbad| Custom Firmware |          1003|9123458981|   Hyderbad|       7.0| BE/BTech| EFU589642|HYBRD9720V|    yes|  no|         208|           235|     10025000000842|1112510000776|1114020000871|         NULL|          NULL| 821163.0|4554 2395 1258 1322|2075 9438 8752 1005|             yes|           117670|             39140|            140621|H.No:1/L,Classic ...|          30-05-2010|NULL|NULL|\n",
      "|        M Vijay|     30|        single|232312|  Wipro|        Hyderbad| Custom Firmware |          1003|9123458985|   Hyderbad|       8.0| BE/BTech|IJP8252380|HYBRD9721A|    yes|  no|         208|          6000|     10025000000843|1112510000777|1114020000872|         NULL|          NULL| 821154.0|5059 2385 1258 1223|2076 9438 8752 1005|             yes|           120188|            183670|            581330|H.No:1/D,Jain soc...|          30-05-2010|NULL|NULL|\n",
      "|        Chaubey|     28|       married| 10025|    IBM|            Pune|             ABAP|          1008|9556811653|  Bangalore|       6.5| BE/BTech|PLI9632589|PUENP7012D|    yes| yes|         210|           287|     10025000000085|1112510000288|         NULL|         NULL| 1111010000759| 821154.0|4453 2385 1258 1223|1318 9438 8752 1005|             yes|             4389|            122493|            371177|H.No:1/K,gymkhana...|          26-08-2008|NULL|NULL|\n",
      "| Shobhit Gautam|     25|       married| 25816|  IGATE|          Mumbai|     MySQL Server|          1011|9123461589|   Hyderbad|       1.0|    MTECH| QSZ126593|MUMBI2370Y|    yes|  no|         208|           320|     10025000001494|1112510001108|         NULL|         NULL|          NULL| 800000.0|5580 2385 1258 1223|2727 9438 8752 1005|             yes|            22966|            221198|            517001|H.No:106/J,shivar...|          13-06-2006|NULL|NULL|\n",
      "|    Sneha Diwan|     27|       married| 25817|  IGATE|          Mumbai|     MySQL Server|          1011|9123461593|   Hyderbad|       2.0|    MTECH|PLI9632600|MUMBI2371T|    yes|  no|         208|           589|     10025000001495|         NULL|         NULL|         NULL|          NULL| 800000.0|5581 2385 1258 1223|2728 9438 8752 1005|             yes|           180615|            264579|            517760|H.No:212/G,gymkha...|          13-06-2006|NULL|NULL|\n",
      "+---------------+-------+--------------+------+-------+----------------+-----------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+-------------+-------------+--------------+---------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Top 10 Bank Balances\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Convert BALANCE to numeric and drop rows with null balances\n",
    "df_with_balance = df.withColumn(\"BALANCE\", col(\"BALANCE\").cast(\"double\")) \\\n",
    "                    .na.drop(subset=[\"BALANCE\"])\n",
    "\n",
    "# Get top 10 accounts by balance\n",
    "top_balances = df_with_balance.orderBy(col(\"BALANCE\").desc()).limit(10)\n",
    "\n",
    "# Show the result\n",
    "top_balances.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df5b4a8-a4f6-47d5-9d52-4071a33fc885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------+------+-------+----------------+--------------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+---------+-------------+--------------+-------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "|       EMP_NAME|EMP_AGE|MARITAL STATUS|EMP ID|COMPANY|COMPANY LOCATION|      SPECIALIZATION|DESIGNATION_ID|  PHONE NO|BANK BRANCH|EXPERIENCE|EDUCATION|  VOTER_ID|  PAN CARD|HOUSING|Loan|LOAN TYPE_ID|REWARD POINTS |SAVINGS ACCOUNT NO.|    CD ACC NO|FD ACC NO|    OD ACC NO|CURRENT ACC NO|BALANCE|     CREDIT CARD NO|      DEBIT CARD NO|INTERNET BANKING|AVG BALANCE YEAR1|AVG BALANCE YEAR 2|AVG BALANCE YEAR 3|ADDRESS OF EMPLOYEES|ACCOUNT OPENING DATE|_c32|_c33|\n",
      "+---------------+-------+--------------+------+-------+----------------+--------------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+---------+-------------+--------------+-------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "|        Hussain|     24|       married| 10031|    IBM|       Bangalore|Business Intellig...|          1008|9583767895|  Bangalore|       2.0| BE/BTech| ALM963256|BJNPA6577N|    yes|  no|         208|            85|     10025000000001|         NULL|     NULL|         NULL|          NULL|  21430|4386 2385 1258 1223|1234 9438 8752 1005|             yes|            12259|             23240|            485793|Shinde wadi,Dadar(E)|          24-01-2010|NULL|NULL|\n",
      "|         Nagesh|     26|        single| 10032|    IBM|       Bangalore|                J2EE|          1008|9546276543|  Bangalore|       3.5| BE/BTech| ALM963257|BJNPA6475K|    yes|  no|         208|            45|     10025000000002|1112510000249|     NULL|1111250000025|          NULL|   2900|4386 2395 1258 1322|1235 9438 8752 1005|             yes|            76382|            354993|            170224|H.No:1/A,Tata Hos...|          26-02-2009|NULL|NULL|\n",
      "|       Preetham|     27|       married| 10033|    IBM|       Bangalore|                ABAP|          1008|9876534563|  Bangalore|       4.5| BE/BTech| ALM963258|BJNPA7541L|    yes| yes|         205|           635|     10025000000003|1112510000250|     NULL|         NULL|          NULL|   2222|4387 2385 1258 1223|1236 9438 8752 1005|             yes|           103845|            275314|            587337|H.No:1/A,Caters R...|          27-02-2009|NULL|NULL|\n",
      "| Rajashekarappa|     28|       married| 10034|    IBM|       Bangalore|                 V&V|          1008|9875674535|  Bangalore|       6.5| BE/BTech| ALM963259|BJNPA8658H|    yes|  no|         208|          1235|     10025000000004|1112510000251|     NULL|         NULL|          NULL| 150681|4388 2385 1258 1223|1237 9438 8752 1005|             yes|           106603|            313436|            448034|H.No:2/D,Dharavi ...|          28-02-2009|NULL|NULL|\n",
      "|        Monappa|     30|        single| 10035|    IBM|       Bangalore|              DotNet|          1008|9845635674|  Bangalore|       7.5| BE/BTech| ZAG614233|BJNPA6875G|     no|  no|         208|          5214|     10025000000005|1112510000252|     NULL|         NULL|          NULL|1878787|4389 2385 1258 1223|1238 9438 8752 1005|             yes|           109832|            141143|             18386|H.No:2/T,Chandvar...|          01-03-2009|NULL|NULL|\n",
      "|          Uthra|     31|       married| 10036|    IBM|       Bangalore|                J2EE|          1008|8765845638|  Bangalore|       8.0| BE/BTech| ZAG614234|BJNPA5476N|    yes|  no|         208|          6357|     10025000000006|1112510000253|     NULL|         NULL|          NULL| 231886|4390 2385 1258 1223|1239 9438 8752 1005|             yes|            58556|            330158|            230416|H.No:1/G,Napoo Ro...|          02-03-2009|NULL|NULL|\n",
      "|Narasimhamurthy|     27|        single| 10037|    IBM|       Bangalore|                ABAP|          1008|7895436370|  Bangalore|       4.5| BE/BTech| ZAG614235|BJNPA1234K|    yes| yes|         211|          8564|     10025000000007|1112510000254|     NULL|         NULL|          NULL| 447787|4387 2395 1258 1322|1240 9438 8752 1005|             yes|           125325|            292568|             79537|H.No:4/S,Bhulabha...|          03-03-2009|NULL|NULL|\n",
      "|         Thorat|     24|      divorced| 10038|    IBM|       Bangalore|                 V&V|          1008|9845352311|  Bangalore|       3.5| BE/BTech| ZAG614236|BJNPA4126J|    yes|  no|         208|           123|     10025000000008|1112510000255|     NULL|         NULL| 1111010000752| 220202|4391 2385 1258 1223|1241 9438 8752 1005|             yes|           185702|            262717|            408536|H.No:3/L,dargah r...|          24-01-2010|NULL|NULL|\n",
      "|        Puvvadi|     25|       married| 10039|    IBM|       Bangalore|              DotNet|          1008|9873452475|  Bangalore|       3.0| BE/BTech| ZAG614237|BJNPA3254B|    yes|  no|         208|             5|     10025000000009|1112510000256|     NULL|         NULL|          NULL|  12180|4392 2385 1258 1223|1242 9438 8752 1005|             yes|            92631|             88007|            474449|H.No:5/S,Moti sha...|          25-06-2012|NULL|NULL|\n",
      "|         Gawand|     23|        single| 10040|    IBM|       Bangalore|              DotNet|          1008|8765341234|  Bangalore|       2.0| BE/BTech| ZAG614238|BJNPA4568V|    yes|  no|         208|          4562|     10025000000010|1112510000257|     NULL|         NULL|          NULL|  59300|4393 2385 1258 1223|1243 9438 8752 1005|             yes|           141313|             93589|            578713|H.No:1/M,jame jam...|          02-07-2008|NULL|NULL|\n",
      "|      Pulathota|     28|      divorced| 10041|    IBM|       Bangalore|Business Intellig...|          1008|9645341234|  Bangalore|       6.0| BE/BTech| ZAG614239|BJNPA7412K|    yes|  no|         208|          2598|     10025000000011|1112510000258|     NULL|         NULL|          NULL|  27000|4394 2385 1258 1223|1244 9438 8752 1005|             yes|           169365|             61047|            348628|H.No:11/H,Worlika...|          25-06-2012|NULL|NULL|\n",
      "|      Manjunath|     29|        single| 10042|    IBM|       Bangalore|                J2EE|          1008|9764531237|  Bangalore|       5.5| BE/BTech| ZAG614240|BJNPA1254H|    yes|  no|         208|          4512|     10025000000012|1112510000259|     NULL|         NULL|          NULL|  39052|4388 2395 1258 1322|1245 9438 8752 1005|             yes|            31883|            149775|            361153|H.No:3/Z,Amarsons...|          25-06-2012|NULL|NULL|\n",
      "|        Kandpal|     30|       married| 10043|    IBM|       Bangalore|                ABAP|          1008|9538675235|  Bangalore|       8.0| BE/BTech| ZAG614241|BJNPA7412M|    yes|  no|         208|           754|     10025000000013|1112510000260|     NULL|         NULL|          NULL|  60000|4395 2385 1258 1223|1246 9438 8752 1005|             yes|            89986|             35919|              5675|H.No:7/T,shivari ...|          25-06-2012|NULL|NULL|\n",
      "|     Pazhanivel|     28|       married| 10044|    IBM|       Bangalore|                 V&V|          1008|9632587414|  Bangalore|       4.5| BE/BTech| ZAG614242|BJNPA2536C|    yes|  no|         208|         12564|     10025000000014|1112510000261|     NULL|         NULL|          NULL|   5000|4396 2385 1258 1223|1247 9438 8752 1005|             yes|           190991|            375462|            401567|H.No:2/S,cloves l...|          25-06-2012|NULL|NULL|\n",
      "|       Chappidi|     27|       married| 10045|    IBM|       Bangalore|              DotNet|          1008|9685324585|  Bangalore|       3.5| BE/BTech| ZAG614243|BJNPA5624K|    yes|  no|         208|         78546|     10025000000015|1112510000262|     NULL|         NULL|          NULL|   1620|4397 2385 1258 1223|1248 9438 8752 1005|             yes|            69000|             76877|            530014|H.No:1/A,Lifestyl...|          25-06-2012|NULL|NULL|\n",
      "|           Jose|     29|       married| 10046|    IBM|       Bangalore|                ABAP|          1008|9632587412|  Bangalore|       6.5| BE/BTech| ZAG614244|BJNPA6548I|    yes|  no|         208|           235|     10025000000016|1112510000263|     NULL|         NULL|          NULL|  22900|4398 2385 1258 1223|1249 9438 8752 1005|              no|           156310|            216081|            444373|H.No:13/R,Queens ...|          25-06-2007|NULL|NULL|\n",
      "|         Dsouza|     35|        single| 10047|    IBM|       Bangalore|                 V&V|          1008|9865321472|  Bangalore|       7.5| BE/BTech| ZAG614245|BJNPA4758O|    yes|  no|         208|          1246|     10025000000017|1112510000264|     NULL|         NULL|          NULL|   1300|4389 2395 1258 1322|1250 9438 8752 1005|              no|            60254|            104765|              6613|H.No:3/A,M G road...|          25-06-2007|NULL|NULL|\n",
      "|      Gangasani|     36|       married| 10048|    IBM|       Bangalore|Business Intellig...|          1008|9874563214|  Bangalore|       7.5| BE/BTech| ZAG614246|BJNPA1597L|    yes|  no|         208|          5236|     10025000000018|1112510000265|     NULL|         NULL|          NULL|   5002|4399 2385 1258 1223|1251 9438 8752 1005|              no|           152654|             36260|            520146|H.No:4/G,sion hos...|          25-06-2007|NULL|NULL|\n",
      "|   Ghattamaneni|     35|       married| 10049|    IBM|       Bangalore|                J2EE|          1008|9632587415|  Bangalore|       6.5| BE/BTech| ALM963256|BJNPA3574M|    yes|  no|         208|          5632|     10025000000019|1112510000266|     NULL|         NULL|          NULL|      0|4400 2385 1258 1223|1252 9438 8752 1005|              no|            60596|            203807|             47661|H.No:5/A,Dumping ...|          25-06-2007|NULL|NULL|\n",
      "|         Ghadge|     27|       married| 10050|    IBM|       Bangalore|                ABAP|          1008|9874563218|  Bangalore|       5.5| BE/BTech|BPL6598412|BJNPA6456Q|    yes|  no|         208|         41235|     10025000000020|1112510000267|     NULL|         NULL|          NULL|  40000|4401 2385 1258 1223|1253 9438 8752 1005|             yes|            78486|            285216|            559623|H.No:1/E,Mill rao...|          25-06-2007|NULL|NULL|\n",
      "+---------------+-------+--------------+------+-------+----------------+--------------------+--------------+----------+-----------+----------+---------+----------+----------+-------+----+------------+--------------+-------------------+-------------+---------+-------------+--------------+-------+-------------------+-------------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partitioned Grouping by Age and Education\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\") \\\n",
    "    .cache()\n",
    "\n",
    "# Filter out nulls for EMP_AGE and EDUCATION\n",
    "filtered_df = df.filter(col(\"EMP_AGE\").isNotNull() & col(\"EDUCATION\").isNotNull())\n",
    "\n",
    "# Group by EMP_AGE and EDUCATION and count\n",
    "result_df = filtered_df.groupBy(\"EMP_AGE\", \"EDUCATION\") \\\n",
    "    .agg(count(\"*\").alias(\"Person_Count\")) \\\n",
    "    .orderBy(\"EMP_AGE\", \"EDUCATION\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6accc9a-2163-4859-bf46-50751e3d927b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|COMPANY      |Account_Holder_Count|\n",
      "+-------------+--------------------+\n",
      "|IGATE        |749                 |\n",
      "|IBM          |601                 |\n",
      "|Wipro        |202                 |\n",
      "|Cognizant    |100                 |\n",
      "|Accenture    |96                  |\n",
      "|Tech Mahindra|64                  |\n",
      "|Infosys      |55                  |\n",
      "|Global Edge  |38                  |\n",
      "|TCS          |32                  |\n",
      "|Seimens      |31                  |\n",
      "|Qualcomm     |30                  |\n",
      "|Cap Gemini   |2                   |\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, length, count, desc\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Company Account Holder Count with Partitioning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV into DataFrame\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\") \\\n",
    "    .cache()\n",
    "\n",
    "# Clean and filter: remove rows with null or empty COMPANY\n",
    "cleaned_df = df.filter(col(\"COMPANY\").isNotNull() & (length(trim(col(\"COMPANY\"))) > 0))\n",
    "\n",
    "# Repartition by COMPANY for optimized processing\n",
    "partitioned_df = cleaned_df.repartition(\"COMPANY\")\n",
    "\n",
    "# Group by COMPANY and count\n",
    "company_counts = partitioned_df.groupBy(\"COMPANY\") \\\n",
    "    .agg(count(\"*\").alias(\"Account_Holder_Count\")) \\\n",
    "    .orderBy(desc(\"Account_Holder_Count\"))\n",
    "\n",
    "# Show result\n",
    "company_counts.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f0d6f9-3c0e-47e0-83d6-e4be0815f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "married,yes,no,914\n",
      "single,yes,no,425\n",
      "divorced,yes,no,204\n",
      "married,no,yes,16\n",
      "single,no,yes,2\n",
      "divorced,no,yes,3\n",
      "married,yes,yes,153\n",
      "single,no,no,39\n",
      "single,yes,yes,54\n",
      "married,no,no,125\n",
      "divorced,no,no,29\n",
      "divorced,yes,yes,36\n",
      ",,,99\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Loan Analysis using RDD\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load the CSV file as text\n",
    "data = sc.textFile(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Extract header and remove it from the data\n",
    "header = data.first()\n",
    "rows = data.filter(lambda line: line != header)\n",
    "\n",
    "# Split each row by comma and extract relevant columns\n",
    "# MARITAL STATUS = index 2, HOUSING = index 14, LOAN = index 15\n",
    "analysis_rdd = rows.map(lambda line: line.split(\",\", -1)) \\\n",
    "    .filter(lambda fields: len(fields) > 15) \\\n",
    "    .map(lambda fields: ((fields[2].strip(), fields[14].strip(), fields[15].strip()), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .map(lambda x: f\"{x[0][0]},{x[0][1]},{x[0][2]},{x[1]}\")\n",
    "\n",
    "# Collect and print the results\n",
    "for line in analysis_rdd.collect():\n",
    "    print(line)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aec165c-6c28-4341-bd57-2e385f02d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+--------------------+\n",
      "|EMP_NAME       |EMP_AGE|Retirement_Category |\n",
      "+---------------+-------+--------------------+\n",
      "|Hussain        |24     |Below Retirement Age|\n",
      "|Nagesh         |26     |Below Retirement Age|\n",
      "|Preetham       |27     |Below Retirement Age|\n",
      "|Rajashekarappa |28     |Below Retirement Age|\n",
      "|Monappa        |30     |Below Retirement Age|\n",
      "|Uthra          |31     |Below Retirement Age|\n",
      "|Narasimhamurthy|27     |Below Retirement Age|\n",
      "|Thorat         |24     |Below Retirement Age|\n",
      "|Puvvadi        |25     |Below Retirement Age|\n",
      "|Gawand         |23     |Below Retirement Age|\n",
      "|Pulathota      |28     |Below Retirement Age|\n",
      "|Manjunath      |29     |Below Retirement Age|\n",
      "|Kandpal        |30     |Below Retirement Age|\n",
      "|Pazhanivel     |28     |Below Retirement Age|\n",
      "|Chappidi       |27     |Below Retirement Age|\n",
      "|Jose           |29     |Below Retirement Age|\n",
      "|Dsouza         |35     |Below Retirement Age|\n",
      "|Gangasani      |36     |Below Retirement Age|\n",
      "|Ghattamaneni   |35     |Below Retirement Age|\n",
      "|Ghadge         |27     |Below Retirement Age|\n",
      "+---------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Retirement Policy Categorization with Names\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Clean and cast EMP_AGE to Integer\n",
    "cleaned_df = df.filter(col(\"EMP_AGE\").isNotNull()) \\\n",
    "    .withColumn(\"EMP_AGE\", col(\"EMP_AGE\").cast(IntegerType()))\n",
    "\n",
    "# Categorize based on age\n",
    "categorized_df = cleaned_df.withColumn(\"Retirement_Category\",\n",
    "    when(col(\"EMP_AGE\") < 55, \"Below Retirement Age\")\n",
    "    .when(col(\"EMP_AGE\").between(55, 60), \"Approaching Retirement\")\n",
    "    .otherwise(\"Eligible for Retirement\")\n",
    ")\n",
    "\n",
    "# Select employee name and retirement category\n",
    "result_df = categorized_df.select(\"EMP_NAME\", \"EMP_AGE\", \"Retirement_Category\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b3a52af-cdd5-48f8-870d-05ef35819833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+-----------+---------------------------+\n",
      "|EMP_NAME       |REWARD_POINTS|Reward_Tier|Offer                      |\n",
      "+---------------+-------------+-----------+---------------------------+\n",
      "|Hussain        |85           |Bronze     |5% off on next purchase    |\n",
      "|Nagesh         |45           |Bronze     |5% off on next purchase    |\n",
      "|Preetham       |635          |Bronze     |5% off on next purchase    |\n",
      "|Rajashekarappa |1235         |Silver     |10% off + free shipping    |\n",
      "|Monappa        |5214         |Gold       |15% off + priority support |\n",
      "|Uthra          |6357         |Gold       |15% off + priority support |\n",
      "|Narasimhamurthy|8564         |Gold       |15% off + priority support |\n",
      "|Thorat         |123          |Bronze     |5% off on next purchase    |\n",
      "|Puvvadi        |5            |Bronze     |5% off on next purchase    |\n",
      "|Gawand         |4562         |Silver     |10% off + free shipping    |\n",
      "|Pulathota      |2598         |Silver     |10% off + free shipping    |\n",
      "|Manjunath      |4512         |Silver     |10% off + free shipping    |\n",
      "|Kandpal        |754          |Bronze     |5% off on next purchase    |\n",
      "|Pazhanivel     |12564        |Platinum   |20% off + VIP lounge access|\n",
      "|Chappidi       |78546        |Platinum   |20% off + VIP lounge access|\n",
      "|Jose           |235          |Bronze     |5% off on next purchase    |\n",
      "|Dsouza         |1246         |Silver     |10% off + free shipping    |\n",
      "|Gangasani      |5236         |Gold       |15% off + priority support |\n",
      "|Ghattamaneni   |5632         |Gold       |15% off + priority support |\n",
      "|Ghadge         |41235        |Platinum   |20% off + VIP lounge access|\n",
      "+---------------+-------------+-----------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Reward Tier Offers\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Clean and cast REWARD POINTS column\n",
    "cleaned_df = df.withColumn(\"REWARD_POINTS\", col(\"REWARD POINTS \").cast(IntegerType()))\n",
    "\n",
    "# Categorize into reward tiers\n",
    "with_tiers = cleaned_df.withColumn(\"Reward_Tier\",\n",
    "    when(col(\"REWARD_POINTS\").between(0, 999), \"Bronze\")\n",
    "    .when(col(\"REWARD_POINTS\").between(1000, 4999), \"Silver\")\n",
    "    .when(col(\"REWARD_POINTS\").between(5000, 9999), \"Gold\")\n",
    "    .when(col(\"REWARD_POINTS\") >= 10000, \"Platinum\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Add personalized offers\n",
    "with_offers = with_tiers.withColumn(\"Offer\",\n",
    "    when(col(\"Reward_Tier\") == \"Bronze\", \"5% off on next purchase\")\n",
    "    .when(col(\"Reward_Tier\") == \"Silver\", \"10% off + free shipping\")\n",
    "    .when(col(\"Reward_Tier\") == \"Gold\", \"15% off + priority support\")\n",
    "    .when(col(\"Reward_Tier\") == \"Platinum\", \"20% off + VIP lounge access\")\n",
    "    .otherwise(\"No offer available\")\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "result_df = with_offers.select(\"EMP_NAME\", \"REWARD_POINTS\", \"Reward_Tier\", \"Offer\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c8dbbad-b7d7-41dd-afc7-5f66c4b25136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|YEAR|Account_Count|\n",
      "+----+-------------+\n",
      "|1900|            1|\n",
      "|2006|          116|\n",
      "|2007|          102|\n",
      "|2008|          181|\n",
      "|2009|          410|\n",
      "|2010|          381|\n",
      "|2011|          351|\n",
      "|2012|          321|\n",
      "|2013|          134|\n",
      "|2014|            1|\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Accounts Opened Per Year with SQL\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"C:\\\\Users\\\\adashrat\\\\Downloads\\\\BANKDATA.csv\")\n",
    "\n",
    "# Filter rows with valid date format (dd-MM-yyyy)\n",
    "valid_date_df = df.filter(col(\"ACCOUNT OPENING DATE\").rlike(\"^\\\\d{2}-\\\\d{2}-\\\\d{4}$\"))\n",
    "\n",
    "# Parse the date\n",
    "df_with_date = valid_date_df.withColumn(\n",
    "    \"ACCOUNT_OPENING_DATE_PARSED\",\n",
    "    to_date(col(\"ACCOUNT OPENING DATE\"), \"dd-MM-yyyy\")\n",
    ")\n",
    "\n",
    "# Create a temporary view for SQL\n",
    "df_with_date.createOrReplaceTempView(\"bankdata\")\n",
    "\n",
    "# Run SQL query to count accounts opened per year\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT year(ACCOUNT_OPENING_DATE_PARSED) AS YEAR, COUNT(*) AS Account_Count\n",
    "    FROM bankdata\n",
    "    WHERE ACCOUNT_OPENING_DATE_PARSED IS NOT NULL\n",
    "    GROUP BY year(ACCOUNT_OPENING_DATE_PARSED)\n",
    "    ORDER BY YEAR\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ce7b2-b29f-4896-8af8-680ea57e1282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
